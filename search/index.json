[{"content":"Mahjong Detection with Yolo Models Introduction Recently I started an interesting endeavor to detect and recognize mahjong hands from a given image and calculate the score of the hand. This idea pops to me because in Riichi Mahjong, scoring is very difficult and requires a lot of familarity with the rules. Especially for beginners, it is very hard to calculate the score of a hand because one has to be really adequate with the concept of Fu and Han.\nThe Han and Fu is different from hand to hand. Fu is particularly difficult to remember. Therefore, I started to think what if I can just take a picture with my phone and the app can tell me the score of the hand. That would be very convenient especially for beginners.\nMy deployed model to try out Other interesting mahjong apps I deployed First Step: Struggle to train locally Found a random repo on github that geneartes mahjong tiles on random background. The quality of the images are not very good, and I trained Yolo v5 and v8 on it, the recognition result is rather terrible. Todo: talk about yolo v5 and v8 Todo: talk about training on Apple silicon. Second Step: Roboflow is the Saver Found really great training data on Roboflow with mahjong tiles. Training on local takes forever but now performs much better. Third Step: Training on Roboflow Thanks to Eli from Roboflow team who geneoursly provided me with 20 free training credits. Hitting mAP, Precision, Recall over 99% but performs terribly on my own mahjong Fourth Step: Training on my own mahjong Take pictures Found human lablellers for cheap on taobao. $0.01 per detection box. Training on Roboflow again Does it much better now. Add screenshots below Step 5: Stretch Goals Mahjong Scoring, talk about different scoring packages I found Add inference code demo How to calulate SHanten etc. Appendix Code Samples InferenceClient 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 import os from inference_sdk import InferenceHTTPClient, InferenceConfiguration #### Inference Engine class MahjongInference: def __init__(self, api_key, model_id, test_image_dir=None): self.test_image_dir = test_image_dir self.api_key = api_key self.model_id = model_id self.image_extensions = [\u0026#39;.jpg\u0026#39;, \u0026#39;.jpeg\u0026#39;, \u0026#39;.png\u0026#39;, \u0026#39;.gif\u0026#39;, \u0026#39;.bmp\u0026#39;, \u0026#39;.tiff\u0026#39;] if test_image_dir: self.image_files = self.get_image_files_from_directory(test_image_dir) self.CLIENT = InferenceHTTPClient( api_url=\u0026#34;https://detect.roboflow.com\u0026#34;, api_key=self.api_key ) self.custom_configuration = InferenceConfiguration(confidence_threshold=0.5, max_detections=14) self.CLIENT.configure(self.custom_configuration) # self.result = None def infer(self): return self.CLIENT.infer(self.image_files, model_id=self.model_id) def infer_single_image(self, image_path): return self.CLIENT.infer([image_path], model_id=self.model_id) def infer_directory(self, directory_path): image_files = self.get_image_files_from_directory(directory_path) return self.CLIENT.infer(image_files, model_id=self.model_id) def get_image_files_from_directory(self, directory_path): files_in_directory = os.listdir(directory_path) return [os.path.join(directory_path, file) for file in files_in_directory if os.path.splitext(file)[1].lower() in self.image_extensions and \u0026#34;annotated\u0026#34; not in file] Hand Scorer 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 from mahjong.hand_calculating.hand import HandCalculator from mahjong.tile import TilesConverter from mahjong.hand_calculating.hand_config import HandConfig from mahjong.meld import Meld from collections import defaultdict from mahjong.shanten import Shanten class MahjongScorer: def __init__(self, player_wind=\u0026#39;east\u0026#39;, round_wind=\u0026#39;east\u0026#39;): self.calculator = HandCalculator() self.config = HandConfig(is_tsumo=False, is_riichi=False, player_wind=player_wind, round_wind=round_wind) self.dora_indicators = [] def update_config(self, **kwargs): for key, value in kwargs.items(): if hasattr(self.config, key): setattr(self.config, key, value) def update_dora_indicators(self, dora_indicators): self.dora_indicators = dora_indicators def _parse_hand_tiles(self, hand): hand_dict = defaultdict(list) for tile in hand: if \u0026#39;m\u0026#39; in tile: hand_dict[\u0026#39;m\u0026#39;].append(tile[0]) elif \u0026#39;p\u0026#39; in tile: hand_dict[\u0026#39;p\u0026#39;].append(tile[0]) elif \u0026#39;s\u0026#39; in tile: hand_dict[\u0026#39;s\u0026#39;].append(tile[0]) elif \u0026#39;z\u0026#39; in tile: hand_dict[\u0026#39;z\u0026#39;].append(tile[0]) sorted_hand = \u0026#39;\u0026#39; for suit in \u0026#39;mpsz\u0026#39;: if suit in hand_dict: sorted_tiles = \u0026#39;\u0026#39;.join(sorted(hand_dict[suit])) sorted_hand += f\u0026#39;{sorted_tiles}{suit}\u0026#39; return sorted_hand def _convert_hand_to_tiles(self, hand, tiles_type): hand_string = self._parse_hand_tiles(hand) if tiles_type == 136: return TilesConverter.one_line_string_to_136_array(string=hand_string) else: return TilesConverter.one_line_string_to_34_array(string=hand_string) def _tile_string_representation(self, hand): return self._parse_hand_tiles(hand) def _calculate_dora_tiles(self): return TilesConverter.one_line_string_to_136_array(string=\u0026#34;\u0026#34;.join(self.dora_indicators)) def _print_verbose(self, hand, result): print(\u0026#34;你的手牌是: \u0026#34;, self._tile_string_representation(hand)) print(f\u0026#34;点数: {result.cost[\u0026#39;total\u0026#39;]}, 番数: {result.han}, 符数: {result.fu}, 牌型: {result.yaku}, 满吗: {result.cost[\u0026#39;yaku_level\u0026#39;]}\u0026#34;) for yaku in result.yaku: print(f\u0026#34;\\t\\t役: {yaku.name}, 役数: {yaku.han_closed}\u0026#34;) for fu_item in result.fu_details: print(fu_item) print(f\u0026#34;Congrats!\\n\u0026#34;) def hand_score(self, hand, win_tile, verbose=True): tiles = self._convert_hand_to_tiles(hand, tiles_type=136) parsed_win_tile = self._convert_hand_to_tiles([win_tile], tiles_type=136)[0] dora_tiles = self._calculate_dora_tiles() result = self.calculator.estimate_hand_value(tiles, parsed_win_tile, config=self.config, dora_indicators=dora_tiles) if not result.error: if verbose: self._print_verbose(hand, result) return result.cost[\u0026#39;total\u0026#39;], result else: if verbose: print(\u0026#34;ERROR:\u0026#34;, result.error) return -1, None def calculate_shanten(self, hand): tiles = self._convert_hand_to_tiles(hand, tiles_type=34) shanten_calculator = Shanten() return shanten_calculator.calculate_shanten(tiles) def _generate_full_tile_set(self): numbered_tiles = [f\u0026#34;{num}{suit}\u0026#34; for suit in \u0026#34;mps\u0026#34; for num in range(1, 10)] honor_tiles = [f\u0026#34;{num}z\u0026#34; for num in range(1, 8)] # Only 1z through 7z exist return numbered_tiles + honor_tiles def calculate_tenpai_tiles(self, hand): full_tile_set = self._generate_full_tile_set() winning_tiles = [] for tile in full_tile_set: score, result = self.hand_score(hand + [tile], win_tile=tile, verbose=False) if score != -1: winning_tiles.append((score, tile, result)) return sorted(winning_tiles, key=lambda k: k[0], reverse=True) def print_possible_wins(self, winning_tiles): if not winning_tiles: print(\u0026#34;\\t\\t无役 别搞了!\u0026#34;) return for score, tile, result in winning_tiles: print(f\u0026#34;\\t\\t{\u0026#39;自摸\u0026#39; if self.config.is_tsumo else \u0026#39;荣和\u0026#39;}: {tile}, 点数: {result.cost[\u0026#39;total\u0026#39;]}, 番数: {result.han}, 符数: {result.fu}, 牌型: {result.yaku}, 满吗: {result.cost[\u0026#39;yaku_level\u0026#39;]}\u0026#34;) for yaku in result.yaku: print(f\u0026#34;\\t\\t\\t役: {yaku.name}, 役数: {yaku.han_open if yaku.han_open else yaku.han_closed}\u0026#34;) for fu_item in result.fu_details: print(\u0026#34;\\t\\t\\t\u0026#34;, fu_item) def list_all_possible_wins(self, hand): print(\u0026#34;听牌! 你的手牌是: \u0026#34;, self._parse_hand_tiles(hand)) config_scenarios = [ (False, False, \u0026#34;\\t如果默听荣和\u0026#34;), (True, False, \u0026#34;\\t如果自摸\u0026#34;), (False, True, \u0026#34;\\t如果立直荣和\u0026#34;), (True, True, \u0026#34;\\t如果立直自摸\u0026#34;) ] original_config = (self.config.is_tsumo, self.config.is_riichi) for is_tsumo, is_riichi, scenario_message in config_scenarios: self.update_config(is_tsumo=is_tsumo, is_riichi=is_riichi) winning_tiles = self.calculate_tenpai_tiles(hand) print(scenario_message) self.print_possible_wins(winning_tiles) self.update_config(is_tsumo=original_config[0], is_riichi=original_config[1]) print(\u0026#34;GLHF\\n\u0026#34;) def calculate_shanten_improving_tiles(self, hand): \u0026#34;\u0026#34;\u0026#34; Calculate all tiles that can improve the hand\u0026#39;s shanten number. This function goes through all possible tiles and checks if adding each tile to the hand would improve the shanten number. \u0026#34;\u0026#34;\u0026#34; current_shanten = self.calculate_shanten(hand) possible_improvements = defaultdict(list) full_tile_set = self._generate_full_tile_set() for tile in full_tile_set: # Create a new hand with the potential tile new_hand = hand + [tile] new_shanten = self.calculate_shanten(new_hand) # Check if the shanten number has been reduced if new_shanten \u0026lt; current_shanten: possible_improvements[new_shanten].append(tile) # Return a sorted list of improvements return possible_improvements def process_hand(self, hand, win_tile=None, dora_indicators=None): if dora_indicators is not None: self.update_dora_indicators(dora_indicators) if len(hand) == 14: self.hand_score(hand, win_tile) elif len(hand) == 13: shanten = self.calculate_shanten(hand) if shanten == 0: self.list_all_possible_wins(hand) else: print(f\u0026#34;Shanten: {shanten}\u0026#34;) print(self.calculate_shanten_improving_tiles(hand)) else: print(\u0026#34;Invalid hand length\u0026#34;) Image Tagger 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 import cv2 import supervision as sv from inference.models.utils import get_roboflow_model class ImageProcessor: def __init__(self, image_files, result): self.image_files = image_files self.result = result self.mahJongScorer = MahjongScorer() def get_scale_factor(self, image_size, reference_size=800, base_scale=0.8, base_thickness=2): # Calculate the scale factor based on the image size compared to a reference size scale_factor = max(image_size) / reference_size scaled_text_scale = max(base_scale * scale_factor, base_scale) # Ensure minimum scale scaled_text_thickness = max(int(base_thickness * scale_factor), base_thickness) # Ensure minimum thickness return scaled_text_scale, scaled_text_thickness def process_images(self): for img, resp in zip(self.image_files, self.result): image = cv2.imread(img) # Get scale and thickness based on image size text_scale, text_thickness = self.get_scale_factor(image.shape[:2]) detections = sv.Detections.from_inference(resp) bounding_box_annotator = sv.BoundingBoxAnnotator(thickness=text_thickness) label_annotator = sv.LabelAnnotator(text_scale=text_scale, text_thickness=text_thickness) # conf_annotator = sv.PercentageBarAnnotator() annotated_image = bounding_box_annotator.annotate(scene=image, detections=detections) annotated_image = label_annotator.annotate(scene=annotated_image, detections=detections) # annotated_image = conf_annotator.annotate(scene=annotated_image, detections=detections) sv.plot_image(annotated_image) hand = sorted([pred[\u0026#39;class\u0026#39;] for pred in resp[\u0026#39;predictions\u0026#39;]], key=lambda x: (x[1], x[0])) print(img, hand, len(hand)) self.mahJongScorer.process_hand(hand, hand[0], []) # except Exception as e: # print(\u0026#34;Cannot score hand or calculaute shanten\u0026#34;, e) # Create an instance of the ImageProcessor class processor = ImageProcessor(engine.image_files, result=engine.infer()) # Call the process_images method to process the images processor.process_images() ","date":"2024-03-23T14:01:37Z","image":"https://blog.xiax.xyz/p/mahjong-detection-with-yolo-draft-work-in-progress/imgs/mahjong_hand_hu8273dd6a4c364f2525716c78928c16fc_280408_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.xiax.xyz/p/mahjong-detection-with-yolo-draft-work-in-progress/","title":"Mahjong Detection with Yolo (Draft, Work in Progress)"},{"content":"How to Design the Image Recommendation Engine for Pinterest Intro We\u0026rsquo;re tasked with designing a scalable image recommendation system for Pinterest. It leverages the latest in big data and machine learning technologies, such as Kafka, Flink, Spark, Hadoop, Iceberg, Airflow, and Presto.\nAssumptions for the Technical Workflow: Pinterest\u0026rsquo;s platform gathers millions of user interactions every day, including image views, saves, comments, and likes. The image recommendation engine must adapt quickly to a user\u0026rsquo;s changing interests and provide real-time, personalized content. Overall Architecture Design 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 User Apps \u0026amp; Web Browsers | | Publish user interactions (clicks, views, etc.) v Kafka | | User interactions stream | v Flink --------------\u0026gt; Perform real-time processing, update recommendation models | | Write Flink results v Hadoop \u0026lt;---\u0026gt; Iceberg \u0026lt;------------- Airflow coordinating batch jobs | | Tables | | | | |\u0026lt;-- Batch Processing \u0026amp; ETL via Spark | | SQL queries for deep analytics v Presto | | Visualization \u0026amp; Data Exploration v Business Analysts \u0026amp; Data Scientists graph TB UA[User Apps \u0026 Web Browsers] --\u003e|Publish user interactions| K[Kafka] K --\u003e|User interactions stream| F[Flink] F --\u003e|Real-time processing| OS[Recommendation System] F --\u003e|Write real-time results| H[Hadoop] SP[Spark for Batch Analysis \u0026 Machine Learning] --\u003e H A[Airflow] --\u003e|Schedule \u0026 orchestrate| SP H --\u003e|Data stored in| I[Iceberg Tables] P[Presto] --\u003e|SQL analytics queries| I B[Business Analysts \u0026 Data Scientists] --\u003e|Visualization \u0026 Data Exploration| P %% Increase fontsize for all nodes classDef default fontsize:20px, color:#000, fill:#fff, stroke:#000; Technical Workflow: 1. Data Ingestion (Apache Kafka): Kafka Producers: Embed in the Pinterest app and web client. They serialize user activities into compact JSON or Avro formats and send events to user-interactions topics, partitioned principally by user ID for efficient stream processing. Kafka Brokers: Set up with topic partitions for redundancy and parallelism, and to ensure high availability and durability of the user interaction events. The partitioning allows for horizontal scalability in the face of increasing data volumes. 2. Real-time Stream Processing (Apache Flink): Streaming Jobs in Flink: These jobs use Flink\u0026rsquo;s rich CEP (Complex Event Processing) library to identify patterns of user interactions. They might capture sequences like frequent views of a particular image category, which triggers an immediate update to the user\u0026rsquo;s real-time recommendation profile. Real-time Recommendation Model Updates: Take advantage of Flink\u0026rsquo;s state management to quickly adjust recommendation models. These models are stored in state backends, with Flink ensuring fault tolerance using its snapshot and checkpoint mechanisms. 3. Batch Processing and Analysis (Apache Spark): Feature Extraction and Machine Learning: Utilize Spark MLlib to periodically train collaborative filtering, clustering, and deep learning models. These models consume significant amounts of image metadata, user demographic information, and historical interaction logs to improve recommendation accuracy. Data Preprocessing and ETL Jobs: Regularly extract, transform, and load fresh data from HDFS into the format required by machine learning models, performed by Spark\u0026rsquo;s SQL and Dataframe/Dataset APIs. 4. Workflow Orchestration (Apache Airflow): Machine Learning Pipeline Coordination: Use Airflow to manage dependencies among batch jobs, which includes pre-processing data, re-training recommendation models, and updating the production model serving layer. Data Pipeline Monitoring: Allows for monitoring and scheduling of regular ETL jobs and ensures that model training occurs on an updated dataset. 5. Data Storage (Hadoop + Apache Iceberg): Raw Data Storage in HDFS: Acts as the central repository store for raw event data, user profiles, and image metadata. Optimized for sequential access patterns and leverages Hadoop\u0026rsquo;s robust data replication for fault tolerance. Curated Data Layers with Iceberg: Iceberg tables store derived views, aggregated metrics, and machine learning features, optimizing the most common access patterns and providing ACID transactions to keep analytical data consistent even during updates. 6. Interactive Querying (Presto): Ad-Hoc Queries and Analysis: Presto allows data scientists to quickly explore datasets and run ad-hoc analytical queries which help in identifying new trends. They can perform joins across various Iceberg tables containing user interactions, image features, and models\u0026rsquo; output scores. 7. Data Lifecycle and Schema Management: Data Retention Policies: Implemented within Kafka and HDFS to manage and age data, ensuring storage costs are optimized without compromising the ability to retrain historical models. Schema Evolution in Iceberg: Provides flexibility in evolving metadata structures and data schema as the formats of user interactions and image representations change over time. 8. Advanced Machine Learning and Analytics (Spark + MLlib): Model Training and Evaluation: Utilizes large-scale, distributed processing capabilities of Spark to handle the computationally intensive task of machine learning model training, feature engineering, and historical data analysis. 9. Monitoring and Governance: Comprehensive Monitoring: Collects metrics and logs from Kafka, Flink, Spark, and Presto jobs using tools like Prometheus and ELK Stack. These metrics are vital for identifying processing bottlenecks and ensuring the reliability of the data pipelines. Data Governance and Compliance: Implemented across all data stores, with policies enforced to maintain data privacy, access controls, audit logging, and lineage tracking through tools like Apache Atlas and Ranger. Technical Detail Followups Kafka Q: How does the Kafka architecture handle failures and ensure message durability? A:\nKafka uses a distributed commit log, where messages are appended and replicated across multiple brokers. Durability is ensured through these replications; if one broker fails, the others can provide the data. Kafka also uses a write-ahead log to disk, so messages are not lost in case of a system crash. A combination of in-sync replicas and acknowledgments from producers upon writes further fortifies the reliability of the message delivery.\nFlink Q: Can you expand on how Flink\u0026rsquo;s state management supports the real-time image recommendation feature? A:\nFlink\u0026rsquo;s state management allows for the retention of relevant context during processing. For example, it can keep track of the current state of a user\u0026rsquo;s interaction pattern or a model\u0026rsquo;s parameters. This state is consistently checkpointed and stored in a persistent storage like HDFS or Amazon S3, so if there\u0026rsquo;s a failure, the application can resume from the latest successful checkpoint, minimizing data loss and computation.\nSpark Q: How does Spark handle the iterative computation required for machine learning algorithms used in image recommendations? A:\nSpark\u0026rsquo;s core abstraction, the Resilient Distributed Dataset (RDD), is well suited for iterative algorithms common in machine learning. This is because once an RDD is loaded into memory, it\u0026rsquo;s kept there for as long as necessary, drastically speeding up iterative data passes essential for algorithms like gradient descent. Spark\u0026rsquo;s MLlib also optimizes algorithms for distributed computing, partitioning the data across nodes to parallelize computations.\nAirflow Q: How does Apache Airflow help managing dependencies in the ETL workflows for the recommendation engine? A:\nIn Airflow, workflows are expressed as DAGs, with each node representing a task and the edges representing dependencies between these tasks. Airflow can manage scheduling and running these tasks based on the specified dependencies, and it supports complex workflows where the execution order of tasks is critical. It also retries failed tasks, sends alerts, and can dynamically adjust workflows based on parameters or external triggers.\nHadoop \u0026amp; Iceberg Q: Describe how Hadoop and Apache Iceberg work together to manage the large datasets in this application. A:\nHadoop provides the distributed storage (HDFS) and computing resources (YARN) needed to manage and process large-scale datasets, while Apache Iceberg integrates with HDFS to manage these datasets as structured tables. Iceberg adds layers like snapshotting, schema evolution, and transactional capabilities on top of the raw file system, which allows for efficient updates and querying of massive tables within HDFS.\nPresto Q: What specific features of Presto make it a fit for the interactive querying requirements of data scientists? A:\nPresto\u0026rsquo;s in-memory distributed query engine is designed for low-latency, interactive data analysis. It supports complex SQL queries, including aggregations, joins, and window functions that are essential for data scientists to explore and analyze big data quickly. Its ability to query data where it lives, without the need for data movement or transformation, streamlines analysis and reduces time to insight.\nData Lifecycle and Schema Management Q: Can you delve into the specifics of how schema evolution is handled in a pipeline with mixed batch and real-time components? A:\nSchema evolution must be managed carefully to ensure compatibility across different data models used by batch and real-time systems. Iceberg helps manage this by allowing schema changes to occur without downtime or requiring a rewrite of data. It maintains a history of schemas and supports schema enforcement and evolution with backward and forward compatibility. Data produced by real-time Flink jobs can evolve, and batch-based Spark jobs can accommodate these changes, as both can interact with the updated schema without interrupt to ongoing operations.\nAdvanced Machine Learning and Analytics Q: Explain the process of feature engineering for images in Spark and how it contributes to recommendation quality. A:\nFeature engineering in Spark for images involves extracting meaningful information from raw image data that can be used to train recommendation models. Using Spark\u0026rsquo;s MLlib, we could perform tasks like resizing images, normalizing pixel values, and extracting color histograms or texture patterns. MLlib can also be used to apply more sophisticated techniques like deep learning to extract higher-level features. These engineered features are critical for training effective recommendation models that can distinguish between different types of images and user preferences.\nMonitoring and Governance Q: How does the monitoring infrastructure support the operational stability of the image recommendation service? A:\nThe monitoring infrastructure collects metrics and logs from all components of the pipeline. With tools like Prometheus, we can track system performance metrics in real time and set up alerts to notify if certain thresholds are exceeded. The ELK Stack is used for log analysis, which helps in diagnosing system problems and understanding user behavior. Together, they ensure the operational stability of the service by allowing us to rapidly identify and address issues as they arise, ensuring a smooth user experience.\nDesign Choice Follow ups Here are more detailed Q\u0026amp;A examples focusing on the context of a real-world Pinterest-like image recommendation system, weighing the pros and cons of utilized technologies, and discussing alternatives:\nKafka Q: Why choose Kafka over other messaging systems for real-time user interaction data ingestion? A:\nKafka is highly scalable and durable, making it ideal for handling the high-volume and high-velocity data streams produced by Pinterest\u0026rsquo;s user interaction events. Its distributed nature and partitioned topics provide fault tolerance and low-latency, which are crucial for real-time recommendation systems.\nCons:\nKafka can be complex to set up and operate, particularly regarding cluster management and monitoring.\nAlternatives:\nApache Pulsar is an alternative that can offer similar scalability and durability, with native support for multi-tenancy and geo-replication, which could be beneficial for a global platform like Pinterest.\nFlink Q: What makes Flink the preferred choice for real-time recommendation updates and not other stream processing frameworks? A:\nFlink\u0026rsquo;s ability to handle complex event processing and maintain rich state management is unmatched. This enables more sophisticated real-time recommendation model updates and user interaction pattern analysis.\nCons:\nFlink\u0026rsquo;s complex event processing capabilities might be overkill for simpler interaction patterns or where latency is less of a concern.\nAlternatives:\nApache Storm is a simpler alternative for stream processing that can be considered if the processing needs are less complex. For tightly integrated Kafka environments, Kafka Streams might suffice for lightweight, real-time processing tasks.\nSpark Q: In the context of feature extraction and machine learning for image recommendations, what are Spark\u0026rsquo;s advantages and disadvantages? A:\nSpark\u0026rsquo;s MLlib for machine learning and its overall ecosystem for big data processing makes it versatile for handling the ETL, feature extraction, and model training workloads. It is efficient at iterative computation, which is essential in training machine learning models.\nCons:\nSpark\u0026rsquo;s in-memory processing model can be expensive, especially when dealing with very large datasets, as it requires a large amount of RAM.\nAlternatives:\nHadoop MapReduce can be a more cost-effective batch processing alternative in environments that are not latency-sensitive. For deep learning specific tasks, frameworks like TensorFlow or PyTorch could be integrated for GPU-accelerated processing.\nAirflow Q: What features does Airflow offer over other workflow management tools? A:\nAirflow provides an expressive framework for defining complex dependencies and schedules in data processing pipelines. Its user-friendly UI and extensive monitoring capabilities make managing workflows much more straightforward.\nCons:\nAirflow can be resource-intensive and has a steeper learning curve, particularly for those unfamiliar with Python.\nAlternatives:\nLuigi is a simpler Python-based workflow engine that might be appropriate for smaller-scale or less complex workflows. Apache NiFi offers a more GUI-driven approach for data flow management and could be preferable for teams looking for less code-intensive solutions.\nHadoop \u0026amp; Iceberg Q: Are there any drawbacks to using Hadoop and Iceberg, and what other systems could be used? A:\nHadoop\u0026rsquo;s HDFS is proven, reliable, and integrates well with big data processing tools. However, it might not be as cost-effective for storage compared to cloud object stores and is complex to manage. Iceberg provides excellent table management features, but is relatively new and has less community support compared to more mature tools.\nCons:\nHadoop cluster requires significant setup and maintenance efforts. Iceberg is not as battle-tested as other formats and may still be evolving.\nAlternatives:\nCloud-based storage options like Amazon S3 or Azure Data Lake Storage offer similar scalability with lower operational overhead. Delta Lake is an alternative to Iceberg that provides similar ACID guarantees and seamless integration with Databricks’ platform.\nPresto Q: Presto is used for interactive querying, but what factors should we consider when choosing it, and are there any competitive technologies? A:\nPresto\u0026rsquo;s distributed architecture allows for fast query execution over large datasets, which is critical for data exploration and timely insights. However, for very large-scale data warehousing tasks and complex query workloads, Presto may not perform as well as dedicated solutions.\nCons:\nPresto may not handle long-running analytical queries as effectively due to its design for ad-hoc querying. Also, it could be less cost-effective for persistent workloads.\nAlternatives:\nApache Drill supports similar SQL queries on large-scale data but can be more forgiving on resource demands. Amazon Athena and Google BigQuery offer serverless querying options for those already invested in the respective cloud ecosystems.\n","date":"2024-03-25T06:23:23-07:00","image":"https://blog.xiax.xyz/p/big-data-system-design-pinterest/Pinterest-2-1024x500_huc282f49b8f48afaa5f37eb45efe62148_460836_120x120_fill_box_smart1_3.png","permalink":"https://blog.xiax.xyz/p/big-data-system-design-pinterest/","title":"Big Data System Design Pinterest"},{"content":"How to Design the Data Infra for Uber Eats Order Tracking Page Intro A real world example to build scalable data analytics with tools like Kafka, Flink, Spark, Presto, Airflow, Iceberg, etc. Let\u0026rsquo;s delve into a more technical scenario that incorporates Kafka, Flink, Spark, Hadoop, Iceberg, Airflow, and Presto into a comprehensive real-time data processing and analytics pipeline:\nAssumptions for the Technical Workflow: An Uber Eats-like app tracks the location of drivers and communicates order details to users. The system must handle real-time processing, such as matching drivers with orders, and longer-term analytical processing, like optimizing driver routes and forecasting delivery times. Overall Architecture Design Simple Architecture Diagram Ascii Style 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 Driver Apps | | Publish GPS locations v Kafka |\\ | | Driver locations stream | | | v | Flink ----------\u0026gt; Compute aggregations | | | Join streams | | Order events | Manage stateful processing | | Stream | v | | Order System \u0026amp; User Apps | | | Write Flink results v | Hadoop \u0026lt;--------/ \u0026lt;-------------- Airflow orchestrating tasks | ^ | | | Data written back by Spark | | | and other batch jobs | v | | Iceberg Tables \u0026lt;---------------------+ | | SQL queries for analytics v Presto | | Visualization \u0026amp; Reporting v Business Apps Mermaid Style graph TB DA[Driver Apps] --\u003e|Publish GPS locations| K[Kafka] K --\u003e|Driver locations stream| F[Flink] K --\u003e|Order events stream| F F --\u003e|Real-time processing| OA[Order System \u0026 User Apps] F --\u003e|Write Flink results| H[Hadoop] SP[Spark for Batch Analysis] --\u003e|Write batch results| H A[Airflow] --\u003e|Schedule \u0026 orchestrate| SP H --\u003e|Data management| I[Iceberg Tables] P[Presto] --\u003e|SQL queries| I B[Business Apps] --\u003e|Visualization \u0026 Reporting| P %% Define styling classDef default fontsize:16px, color:#fff, fill:#000, stroke:#fff, stroke-width:2px; linkStyle default stroke:white,stroke-width:1px; Technical Workflow: 1. Data Ingestion (Apache Kafka): Kafka Producers: The driver app constantly publishes GPS coordinates to a Kafka topic driver-locations, using a producer client library. Each message might be keyed by driver ID to ensure location updates for a given driver go to the same partition for order. Kafka Consumers: Other systems subscribe to the driver-locations topic to receive updates. Additionally, consumer applications are set up for topics like order-events that handle order placement, acceptance, and completions. Kafka topics are partitioned and replicated to ensure scalability and fault tolerance. 2. Real-time Stream Processing (Apache Flink): Flink Jobs: Flink jobs are configured to subscribe to driver-locations and order-events. They perform tasks such as: Windowed aggregations to compute the latest location of each driver. Join operations between driver locations and pending orders to facilitate real-time matching and dispatching. Stateful processing for ETA calculations and sending notifications to customers via external systems when a driver is nearby. 3. Batch Processing and Analysis (Apache Spark): Spark Jobs: Scheduled batch Spark jobs are written in Scala or Python and deal with complex analytical tasks, including: Training machine learning models on historical delivery data to predict delivery times (using MLlib). Aggregating delivery data nightly to create performance metrics per driver or per region. 4. Workflow Orchestration (Apache Airflow): Airflow DAGs (Directed Acyclic Graphs): Define the dependent jobs and workflows that orchestrate processing tasks. An Airflow DAG might include: A task to run a Spark job that rebuilds machine learning models. Another task to batch-process the day\u0026rsquo;s data and update Iceberg tables. Scheduling and triggering Flink jobs for real-time streaming when needed. 5. Data Storage (Hadoop + Apache Iceberg): HDFS: Raw data from Kafka is landed into HDFS, and processed data is stored there for long-term historical analysis. Iceberg Tables: Transactional data tables managed by Iceberg provide consistent snapshots of large-scale datasets and support schema evolution for the raw Kafka data and the results of Spark jobs. 6. Interactive Querying (Presto): Presto SQL Queries: Analysts use Presto to perform SQL queries on stored Hadoop data and Iceberg tables to gain insights or create real-time dashboards. Example queries might include: SQL to join today\u0026rsquo;s real-time data with historical trends. Aggregation queries to analyze delivery efficiency across different areas. 7. Data Lifecycle and Schema Management: Iceberg table partitioning is configured for efficient data retrieval, e.g., by time window or geographical region. Data retention policies are configured in Kafka to govern how long data is kept, considering storage costs and compliance requirements. Iceberg\u0026rsquo;s schema evolution allows for seamless transitions when altering data structures or incorporating new data sources. 8. Advanced Machine Learning and Analytics (Spark + MLlib): Historical data in Iceberg tables is used to train and refine predictive models for delivery ETA, taking advantage of features like lookback windows and seasonal trends. Airflow DAGs keep model training and evaluations consistent, reproducible, and on schedule. 9. Monitoring and Governance: Comprehensive logging is set up for all components, including Kafka brokers, Flink job managers, and Spark driver/executor processes. A monitoring solution (such as Prometheus with Grafana) watches over the health and performance of the infrastructure, ensuring SLAs are met. Data governance is enforced via policies and access controls for sensitive data in HDFS and Iceberg, audited by tools such as Apache Ranger. The synergy in such a technical workflow allows an Uber Eats-like service to leverage strengths from each tool — Kafka for ingestion, Flink for real-time processing, Spark for batch processing and ML, Airflow for orchestration, Iceberg for managing large-scale data tables, and Presto for interactive querying. This enables real-time communication to users and drivers while also providing a platform for advanced analytics and strategic decision-making based on historical and current data.\nTechnical Detail Followups Kafka Q: How do you ensure data consistency in your Kafka cluster?\nA: Kafka ensures data consistency by replicable partitions, where each partition has one leader and multiple in-sync replicas (ISRs). The leader handles all the read and write requests for that partition, and ISRs replicate the leader\u0026rsquo;s data. Consumers typically read from the leader to ensure they receive the most up-to-date records. Q: Can you explain the role of Zookeeper in a Kafka ecosystem?\nA: Zookeeper plays a critical role in Kafka\u0026rsquo;s architecture; it manages and coordinates Kafka brokers. It\u0026rsquo;s responsible for leader election for partitions, managing configuration information, and maintaining a list of Kafka brokers and topics, as well as their status. Flink Q: What advantages does Flink provide when dealing with stateful computations?\nA: Flink offers robust state management and fault tolerance for stateful computations in streaming data. It maintains a consistent snapshot of all state throughout an application’s execution, which can be recovered in case of a failure. Flink\u0026rsquo;s checkpointing and savepoints feature are key for ensuring exactly-once processing semantics. Q: How does Flink handle event time and out-of-order events?\nA: Flink features an event time concept that deals with the time at which events actually occurred, as opposed to when they are processed by the system. With watermarks, which are special types of events that specify the progress of event time, Flink is capable of handling out-of-order events or late-arriving data. Spark Q: What is the difference between RDDs, DataFrames, and Datasets in Spark?\nA: RDD (Resilient Distributed Dataset) is the fundamental data structure in Spark - an immutable distributed collection of objects that can be processed in parallel. DataFrames are a collection of RDDs, but with a schema, which provides a higher level abstraction and optimization opportunities through Catalyst optimizer. Datasets are a type-safe version of DataFrames, allowing users to work with strongly-typed data collected as JVM objects. Q: How does Spark achieve fault tolerance?\nA: Spark achieves fault tolerance through its immutable data structure and lineage graph. In case of a partition failure, Spark can recompute the lost partition by replaying the transformations used to build the lineage of the RDD. This significantly reduces the need for data replication for fault tolerance purposes. Hadoop Q: What are some of the core components of a Hadoop ecosystem?\nA: The core of Hadoop consists of the storage layer, HDFS (Hadoop Distributed File System), for reliable storage of massive datasets, and YARN (Yet Another Resource Negotiator), for cluster resource management, and the MapReduce programming model for processing large datasets in parallel. Q: How is data stored and processed in Hadoop?\nA: In Hadoop, data is stored in HDFS across a distributed file system that spans the nodes in a Hadoop cluster. Data is processed using a MapReduce job where the data is first mapped, sorted/shuffled, and then reduced to produce an output - each of these steps can run in parallel across the cluster\u0026rsquo;s nodes. Iceberg Q: What is Apache Iceberg and how does it help with managing large data sets?\nA: Apache Iceberg is a table format for large data sets that enables high-performance queries in distributed computing environments. It adds more structure and optimization to data storage, providing benefits like snapshot isolation, schema evolution, and efficient querying through hidden partitioning. Q: Can Iceberg tables work with both batch and streaming data?\nA: Yes, Iceberg tables are designed to support both batch and stream processing workloads seamlessly. They can be read and written to using traditional batch operations or incrementally using streaming workloads, thus, providing flexibility and ensuring up-to-date views of data. Airflow Q: What are the benefits of using Apache Airflow for workflow orchestration?\nA: Apache Airflow provides a highly customizable platform for scheduling and coordinating complex data pipelines. It enables developers to define workflows as code, which allows for easy versioning, testing, and reusability. Its rich user interface aids in visualizing workflows, monitoring job progress, and diagnosing issues. Q: How do you ensure that a task in Airflow is idempotent?\nA: To make sure that a task in Airflow is idempotent - producing the same result regardless of how many times it’s executed - you\u0026rsquo;d structure your tasks so that the output depends solely on the inputs and the code at execution time. Also, using Airflow\u0026rsquo;s extensive logging and external systems for state management can help ensure idempotency. Presto Q: Why would you use Presto over other SQL-on-Hadoop engines?\nA: Presto is designed for low-latency queries and is generally faster than other SQL-on-Hadoop engines due to its distributed architecture and query execution strategies. Unlike others, Presto does not use MapReduce; it processes data using an in-memory pipeline execution model which is significantly faster. Q: How does Presto interact with different data sources, such as Iceberg or Kafka?\nA: Presto has a connector architecture that allows it to interact with various data sources. For Iceberg, the Presto Iceberg connector provides support for reading from and writing to Iceberg tables. For Kafka, the Presto Kafka connector allows Presto to query Kafka topics directly as if they were tables. This flexibility makes it easy to combine and query data across different storage systems. Design Choice Follow ups Kafka Q: Why Kafka for tracking driver locations in real time and not other systems like RabbitMQ or ActiveMQ?\nA: Kafka is built for high throughput and is capable of handling the vast volume of location updates generated by drivers—potentially millions of messages per second. Unlike RabbitMQ or ActiveMQ, Kafka excels at handling large streams of data efficiently, making it ideal for real-time analytics use cases. Also, Kafka\u0026rsquo;s durable storage model allows us to reprocess historical data if needed.\nPros: - High throughput and scalability. - Built-in partitioning and replication. - Fault tolerance.\nCons: - Complexity in setup and management. - Broker-centric storage with less queuing flexibility.\nAlternatives: - RabbitMQ or Pulsar for simpler setups or additional messaging features but with potential scalability limitations.\nFlink Q: Why use Apache Flink for real-time processing of driver locations?\nA: Flink provides low-latency, accurate stream processing, which is critical for the driver location updates to reach customers in a timely manner. It\u0026rsquo;s also good at handling stateful computations, like aggregating driver locations over windows of time. Apache Flink was chosen over alternatives like Apache Storm due to its greater throughput and ease of state management. However, we also considered Apache Kafka Streams for its tight integration with Kafka, but Flink offered more advanced windowing and state management features.\nPros: - High performance and low latency. - Advanced state management. - Effective event-time handling.\nCons: - Complexity for simpler stream processing needs. - Possible overkill for less demanding tasks.\nAlternatives: - Apache Storm for competent stream processing or Kafka Streams for Kafka-centric applications.\nSpark Q: Why is Apache Spark preferred for batch processing and ML over other frameworks?\nA: Spark provides a robust MLlib for machine learning, which we use to predict delivery times. It\u0026rsquo;s optimized for iterative algorithms, such as those used in machine learning, and its in-memory computing capabilities speed up processing. This makes it highly effective for our driver location data analysis. An alternative could be Hadoop MapReduce, but it\u0026rsquo;s a lot slower and less suited for complex analytics and ML tasks that Spark excels at.\nPros: - Fast performance with in-memory computing. - Comprehensive MLlib for machine learning tasks. - Unified APIs for diverse workloads.\nCons: - Resource-intensive, especially for memory.\nAlternatives: - Hadoop MapReduce for cost-effective batch processing or cloud solutions like AWS EMR for managed services.\nHadoop (HDFS) Q: Why prefer HDFS over cloud object storage like Amazon S3?\nA: HDFS offers high throughput and is well-suited for the big data ecosystem, particularly with other Hadoop components. It integrates seamlessly with our data processing engines like Spark and our metadata management through Iceberg. While cloud object storage solutions are great for scalability and cost-effectiveness, HDFS gives us more control over our data locality and performance, which is important for our time-sensitive analytical workloads.\nPros: - Smooth integration with Hadoop-based tools. - High throughput and performance.\nCons: - Complexity in management and operations. - Less elasticity than cloud storage.\nAlternatives: - Cloud solutions like Amazon S3 for scalability and simple operation.\nIceberg Q: How does Apache Iceberg improve large data table management and what are other options?\nA: Iceberg offers reliable, scalable management of big data tables, addressing issues with older file formats like concurrency, versioning, and schema evolution. Iceberg integrates well with Spark and Flink, allowing us to easily manage table formats on HDFS. One alternative is Apache Hive, but it has limitations with metadata scaling which Iceberg has been designed to overcome.\nPros: - Efficient metadata management. - Seamless schema evolution. - Performance optimization through hidden partitioning.\nCons: - Less mature compared to formats like Parquet.\nAlternatives: - Apache Hive for a traditional data warehouse approach, or Delta Lake for ACID transactions and building data pipelines.\nAirflow Q: Why Airflow over other workflow tools?\nA: Airflow excels at defining, scheduling, and monitoring complex data pipelines. Its ability to code workflows as Directed Acyclic Graphs (DAGs) offers flexibility and transparency we need. Moreover, Airflow\u0026rsquo;s user interface and community support are excellent. As an alternative, we considered Luigi and Apache NiFi, but Airflow\u0026rsquo;s abilities to integrate with our data stack and intuitive UI made it the ideal choice for us.\nPros: - Flexibility with a Python-based programming model. - Rich UI for management and monitoring. - Broad integration with data processing tools.\nCons: - Potential resource intensity for simple needs. - Learning curve for non-Python familiar developers.\nAlternatives: - Luigi for simpler Python-centric workflows or Apache NiFi for a more visual-centric approach.\nPresto Q: Why Presto for interactive querying against other technologies?\nA: Presto\u0026rsquo;s distributed in-memory architecture enables us to perform high-speed, ad-hoc queries directly on our data lake without data movement. It\u0026rsquo;s flexible and supports a wide range of data sources, maintaining high performance for interactive analytics. We considered Apache Drill, but Presto\u0026rsquo;s SQL compatibility and performance made it the better fit for our diverse data requirements.\nPros: - Fast querying over various data sources. - Strong SQL support for diverse BI tools.\nCons: - Costly for large datasets due to in-memory processing. - Suboptimal for extended, complex queries.\nAlternatives: - Apache Drill for similar querying capabilities or managed cloud services like Amazon Athena for less operational involvement.\n","date":"2024-03-25T05:40:03-07:00","image":"https://blog.xiax.xyz/p/big-data-system-design-uber-eats/Uber_Eats_Logo_hu9919f7b3f46635b6673d32d5992520f9_38864_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.xiax.xyz/p/big-data-system-design-uber-eats/","title":"Big Data System Design Uber Eats"},{"content":"The Problem \u0026amp; Solution Behind Iceberg\u0026rsquo;s Origin 💡 — What’s the problem and Why Iceberg? What is a data lake? A data lake is a central location that holds a large amount of data in its native, raw format. Compared to a hierarchical data warehouse, which stores data in files or folders, a data lake uses a flat architecture and object storage to store the data.‍ Object storage stores data with metadata tags and a unique identifier, which makes it easier to locate and retrieve data across regions, and improves performance. By leveraging inexpensive object storage and open formats, data lakes enable many applications to take advantage of the data. What’s a table format? A way to organize a datalake’s files to present them as a single table A way to answer the question — what data is in this table So user can query a bunch of files in a data lake (a bunch of files) just like a database or data warehouse Extension on datalake https://www.databricks.com/discover/data-lakes The old de-facto standard — Hive Origin of Hive Originated from Facebook. In early days of datalakes, engineers have to write map reduce java code to interact with their Hadoop cluster. Hive is create to make life easier for people, instead of having to write java code, Hive translate the SQL into MapReduce scripts. To do that, we need a way to recognize files as tables. The approach Hive took is a directory-based approach. Where it would just treat a folder on a Hadoop cluster as a table. And subfolders will be partitions. Pros of Hive More efficient access patterns than full-table scans for every query (Better latency \u0026amp; Cost) Works with basically every engine since it’s been the de-facto standard for so long. File format agnostic Atomically update a whole partition (ACID guanrantee) Single, central answer to “what data is in this table” for the whole datalake ecosystem Cons of Hive Smaller updates are very inefficient No way to change data in multiple partitions safely In practice, Unsafe concurrent partition modifications All of the directory listings needed for large tables take a long time. Extra effort needed to speed up a query: indexing, more processing power, materializing Users have to know the physical Layout of the table to take advantage of performance benefits of partitioning Hive table statics are often stale, and updating statics(with Analyze) is very slow How to resolve the issues with Hive? Netflix having issues with Hive and in need of a new table format.\nNetflix’s Goals Table correctness/consistency Faster query planning and execution Allow users to not worry about the physical layout of the table Table evolution support. (Flexible schema, partitioning changes) Achieve all above at scale. The answer — Iceberg and what is it What is Iceberg? ✅ Table format specification A set of APIs and libraries for interaction with that specification These libraries are leveraged in other engines and tools that allow them to interact with Iceberg tables What Iceberg is not? ❌ Not a storage engine Not an execution engine Not a service Design Benefits of Iceberg Efficiently make smaller updates\nMaking changes at the file level instead of directory level Snapshot Isolation for transactions\nReads and writes don’t interfere with each other and all writes are atomic Anytime the table changes, a snapshot is created. Any read will be on the same latest snapshot Any write will result in a sequence of write Concurrent writes Faster planning and execution (Fresh metadata and statistics)\nList of files defined on the write-side Column stats in manifest files on the write-side Reliable metrics for CBOs\nDone on write instead of “infrequent” expensive read job (Analyze) Abstract the physical, expose a logical view\nHidden partitioning, ppl don’t need to care how the files are stored physically, they just need to know the logical layout as it’s stored in the manifest (list of files and organization of these files) Enables Compaction built-in tools to optimize small files comes from streaming → better performance Tables can change over time (schema and partitions) Can rollback snapshots Rich schema evolution support\nAll engines see changes immediately and can work together\nCommunity\nReference: Apache Iceberg Tutorial: Learn the Problem \u0026amp; Solution Behind Iceberg\u0026rsquo;s Origin Story Databrick\u0026rsquo;s take on datalakes Databrick\u0026rsquo;s take on materialized views ","date":"2024-03-24T02:42:56-07:00","image":"https://blog.xiax.xyz/p/iceberg-101-chapter-1-the-problem-solution-behind-icebergs-origin/iceberg_hu636f1282c24300abf35562f2ec8af016_13976_120x120_fill_q75_h2_box_smart1_2.webp","permalink":"https://blog.xiax.xyz/p/iceberg-101-chapter-1-the-problem-solution-behind-icebergs-origin/","title":"Iceberg 101 - Chapter 1 The Problem \u0026 Solution Behind Iceberg's Origin"},{"content":"立直麻将 牌效率学习 (门清一般型) (大概)很好懂的牌效率\nChapter 1 数牌: 顺子和搭子 Basics 面子 = 顺子+刻子 顺子 \u0026raquo; 刻字 顺子牌效率搭子 → 可以变成顺子的牌 两面 \u0026raquo; 崁张 \u0026raquo; 边张 数字强度 34567 \u0026gt; 28 \u0026gt; 19\nChapter 2 字牌 序盘处理 字牌组成面子的能力弱于数牌 字牌出现的数量越多, 价值急剧下降 单张字牌的价值 比如 1,4 6,9 这样的组合, 属于有效牌重复, 价值甚至低于客风牌\n役牌的价值一般高于19牌, 如果需要役则价值更高\nChapter 3 对子 Chapter 4 向听数, 有效牌, 五种听牌型 向听数\n进张 → 可以使进入下一个向听阶段的牌\n改良 → 使进张牌数增加\n有效牌 = 进张+改良\n听牌型\n两面听 → 8张 双碰(两对) / 崁张 / 边张 → 4张 单骑 → 3张, 但是转张自由度很高 复合听牌型\n双复合\n双单骑\n螺丝型\n顺子附近的数牌都可以让我们转更好的听牌型\n复合三面听 (暗刻复合型就很舒服\n总结\nChapter 5 对子复合听牌, 补强牌 ","date":"2024-03-24T03:22:16-07:00","image":"https://blog.xiax.xyz/p/%E6%97%A5%E9%BA%BB-%E7%89%8C%E6%95%88%E7%8E%87%E5%AD%A6%E4%B9%A0/imgs/Untitled_4_hu0e8d761353474b88d3b7cf36e300ef14_3311421_120x120_fill_box_smart1_3.png","permalink":"https://blog.xiax.xyz/p/%E6%97%A5%E9%BA%BB-%E7%89%8C%E6%95%88%E7%8E%87%E5%AD%A6%E4%B9%A0/","title":"日麻 牌效率学习"},{"content":"","date":"2024-03-23T14:01:53Z","image":"https://blog.xiax.xyz/p/comparing-dockerized-app-deployment-platform-to-be-written/docker_hu151b7a3df04e96b7b9cfd151bae6df4e_206098_120x120_fill_box_smart1_3.png","permalink":"https://blog.xiax.xyz/p/comparing-dockerized-app-deployment-platform-to-be-written/","title":"Comparing Dockerized App Deployment Platform (To be Written)"},{"content":"Simplify Debts Introduction How to recreate the simplify debt from split wise\nWhat is Simplify Debts? Code Demo DFS Approach 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 from collections import defaultdict import math \u0026#39;\u0026#39;\u0026#39; O(n!) DFS Alogirthm \u0026#39;\u0026#39;\u0026#39; class Solution: def minTransfers(self, input_transactions): balance = defaultdict(int) transactions = [] for trans in input_transactions: amount = trans[\u0026#39;amount\u0026#39;] payer = trans[\u0026#39;payer\u0026#39;] payees = trans[\u0026#39;payees\u0026#39;] # Split the transaction amount among the payees (excluding the payer) per_person = round(amount / len(payees)) for payee in payees: transactions.append((payer, payee, per_person)) for from_, to, amt in transactions: balance[to] += amt balance[from_] -= amt # Filter out zero balances and create a list of non-zero balances debts = list(filter(lambda x: x != 0, balance.values())) people = list(filter(lambda x: balance[x] != 0, balance.keys())) # Helper function to find the min transactions to settle starting from \u0026#39;index\u0026#39; def backtrack(index): # Skip already settled accounts while index \u0026lt; len(debts) and debts[index] == 0: index += 1 if index == len(debts): return [], 0 min_txns = math.inf min_trans_list = [] for j in range(index + 1, len(debts)): # Pruning. debts[j] must be non zero and of different sign if debts[j] * debts[index] \u0026lt; 0: # Temporarily settle the debt debts[j] += debts[index] next_trans_list, num_txns = backtrack(index + 1) # If better solution found, update the result if num_txns + 1 \u0026lt; min_txns: min_txns = num_txns + 1 amt = abs(debts[index]) # Determine the payer and payee based on the sign of debts[index] transaction = f\u0026#34;{people[index] if debts[index] \u0026gt; 0 else people[j]} pays {people[j] if debts[index] \u0026gt; 0 else people[index]} ${amt}\u0026#34; # Include current transaction in the list min_trans_list = next_trans_list + [transaction] # Backtrack the temporary change debts[j] -= debts[index] return min_trans_list, min_txns transactions, min_txns = backtrack(0) return transactions, min_txns # Example usage: input_transactions = [ {\u0026#39;payer\u0026#39;: \u0026#39;xia\u0026#39;, \u0026#39;amount\u0026#39;: 15, \u0026#39;payees\u0026#39;: [\u0026#39;xia\u0026#39;, \u0026#39;hao\u0026#39;]}, {\u0026#39;payer\u0026#39;: \u0026#39;hao\u0026#39;, \u0026#39;amount\u0026#39;: 62, \u0026#39;payees\u0026#39;: [\u0026#39;xia\u0026#39;, \u0026#39;hao\u0026#39;]} ] solution = Solution() final_transactions = solution.minTransfers(input_transactions) print(final_transactions) Code Demo Graph Approach 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 import sys import re import argparse from typing import List, Dict, Tuple class NodeError(Exception): pass class EdgeException(Exception): pass class Edge: def __init__(self, start_node: str, end_node: str, weight: float): self.start_node = start_node self.end_node = end_node self.weight = weight def to_graphviz_string(self) -\u0026gt; str: return f\u0026#39;{self.start_node} -\u0026gt; {self.end_node} [ label=\u0026#34;{round(self.weight, 2)}\u0026#34; ];\u0026#39; def __str__(self) -\u0026gt; str: return f\u0026#39;{self.start_node} -\u0026gt; {self.end_node}: {round(self.weight, 2)}\u0026#39; def normalize(self): if self.weight \u0026lt; 0: self.weight *= -1 self.start_node, self.end_node = self.end_node, self.start_node def print_edges(edges: List[Edge], graphviz: bool): if graphviz: print(\u0026#34;digraph G {\u0026#34;) for edge in edges: edge.normalize() print(edge.to_graphviz_string() if graphviz else str(edge)) if graphviz: print(\u0026#34;}\u0026#34;) def add_weight(weights: Dict[str, float], node_name: str, weight_delta: float): weights[node_name] = weights.get(node_name, 0) + weight_delta def sort_weights(weights: Dict[str, float]) -\u0026gt; List[Tuple[float, str]]: return sorted([(value, key) for key, value in weights.items()]) def get_node_weights(edges: List[Edge]) -\u0026gt; Dict[str, float]: weights = {} for edge in edges: add_weight(weights, edge.end_node, edge.weight) add_weight(weights, edge.start_node, -edge.weight) return weights def find_greater_weight(weight_comp: float, weights: Dict[str, float]) -\u0026gt; str: for node, weight in weights.items(): if weight \u0026gt;= weight_comp: return node return None def weights_to_edges(sorted_weights: List[Tuple[float, str]], weights: Dict[str, float]) -\u0026gt; List[Edge]: edges = [] for i in range(len(sorted_weights) - 1): current_node = sorted_weights[i][1] current_weight = weights[current_node] if current_weight == 0: continue transact = abs(current_weight) target = find_greater_weight(transact, weights) if target is None: target = sorted_weights[i + 1][1] edges.append(Edge(current_node, target, transact)) weights[target] += current_weight return edges def remove_zero_weights(weights: List[Tuple[float, str]]): return [w for w in weights if w[0] != 0] def split_star_nodes(edges: List[Edge], empty_nodes: List[str], verbose: bool) -\u0026gt; List[Edge]: nodes = list(set(empty_nodes + [edge.start_node for edge in edges if edge.start_node != \u0026#34;*\u0026#34;] + [edge.end_node for edge in edges if edge.end_node != \u0026#34;*\u0026#34;])) if verbose: print(f\u0026#34;Found these {len(nodes)} unique nodes: {nodes}\u0026#34;) new_edges = [] for edge in edges: if edge.start_node == \u0026#34;*\u0026#34;: new_edges.extend( Edge(node, edge.end_node, edge.weight / len(nodes)) for node in nodes if node != edge.end_node ) elif edge.end_node == \u0026#34;*\u0026#34;: new_edges.extend( Edge(edge.start_node, node, edge.weight / len(nodes)) for node in nodes if node != edge.start_node ) else: new_edges.append(edge) return new_edges SEARCH_COMMENT = re.compile(\u0026#34;^(#| *$)\u0026#34;) SEARCH_EDGE = re.compile(\u0026#34;(\\w+|\\*) *-\u0026gt; *(\\w+|\\*): *([0-9]+(\\.[0-9]+)?)\u0026#34;) SEARCH_NODE = re.compile(\u0026#34;^(\\w+)$\u0026#34;) def parse_edge(line: str) -\u0026gt; Edge: m = SEARCH_EDGE.match(line) if m: start_node, end_node, weight = m.groups()[:3] return Edge(start_node, end_node, float(weight)) raise EdgeException(\u0026#34;Invalid input line\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: argparser = argparse.ArgumentParser() argparser.add_argument(\u0026#34;-g\u0026#34;, \u0026#34;--graphviz\u0026#34;, action=\u0026#39;store_true\u0026#39;) argparser.add_argument(\u0026#34;-v\u0026#34;, \u0026#34;--verbose\u0026#34;, action=\u0026#39;store_true\u0026#39;) argparser.add_argument(\u0026#34;filename\u0026#34;, nargs=\u0026#39;?\u0026#39;) args = argparser.parse_args() edges = [] empty_nodes = [] with open(args.filename) if args.filename else sys.stdin as input_file: for i, line in enumerate(input_file, 1): try: edges.append(parse_edge(line)) except EdgeException: if SEARCH_NODE.match(line): empty_nodes.append(line.strip()) elif not SEARCH_COMMENT.match(line): print(f\u0026#34;Invalid input on line {i}: {line.strip()}\u0026#34;, file=sys.stderr) sys.exit(1) edges = split_star_nodes(edges, empty_nodes, args.verbose) weights = get_node_weights(edges) sorted_weights = sort_weights(weights) sorted_weights = remove_zero_weights(sorted_weights) # Optionally perform a consistency check if verbose if args.verbose and sorted_weights: assert round(sum(weight for weight, _ in sorted_weights), 10) == 0.0 print(\u0026#34;Node weights: \u0026#34;, sorted_weights) edges = weights_to_edges(sorted_weights, weights) if args.verbose and edges: total_money_transacted = sum(edge.weight for edge in edges) print(f\u0026#34;Total money transacted: {total_money_transacted}\u0026#34;) print_edges(edges, args.graphviz) Github Link ","date":"2024-03-23T14:01:53Z","image":"https://blog.xiax.xyz/p/how-to-recreate-split-wises-simplify-debt-feature-draft-work-in-progress/split_wise_hud6757fae20a87920fc6a8f24d3f32c68_10011_120x120_fill_box_smart1_3.png","permalink":"https://blog.xiax.xyz/p/how-to-recreate-split-wises-simplify-debt-feature-draft-work-in-progress/","title":"How to recreate Split Wise's Simplify Debt Feature? (Draft, Work in Progress) "},{"content":"System Design Interview Framework from Alex Xu Step 1 - Understand the problem and establish design scope In a system design interview, giving out an answer quickly without thinking gives you no bonus points. Answering without a thorough understanding of the requirements is a huge red flag as the interview is not a trivia contest. There is no right answer.\nSo, do not jump right in to give a solution. Slow down. Think deeply and ask questions to clarify requirements and assumptions. This is extremely important.\nAs an engineer, we like to solve hard problems and jump into the final design; however, this approach is likely to lead you to design the wrong system. One of the most important skills as an engineer is to ask the right questions, make the proper assumptions, and gather all the information needed to build a system. So, do not be afraid to ask questions.\nWhen you ask a question, the interviewer either answers your question directly or asks you to make your assumptions. If the latter happens, write down your assumptions on the whiteboard or paper. You might need them later. What kind of questions to ask? Ask questions to understand the exact requirements. Here is a list of questions to help you get started:\nWhat specific features are we going to build? • How many users does the product have? How fast does the company anticipate to scale up? What are the anticipated scales in 3 months, 6 months, and a year? What is the company’s technology stack? What existing services you might leverage to simplify the design? Step 2 - Propose high-level design and get buy-in In this step, we aim to develop a high-level design and reach an agreement with the interviewer on the design. It is a great idea to collaborate with the interviewer during the process.\nCome up with an initial blueprint for the design. Ask for feedback. Treat your interviewer as a teammate and work together. Many good interviewers love to talk and get involved. Draw box diagrams with key components on the whiteboard or paper. This might include clients (mobile/web), APIs, web servers, data stores, cache, CDN, message queue, etc. Do back-of-the-envelope calculations to evaluate if your blueprint fits the scale constraints. Think out loud. Communicate with your interviewer if back-of-the-envelope is necessary before diving into it. If possible, go through a few concrete use cases. This will help you frame the high-level design. It is also likely that the use cases would help you discover edge cases you have not yet considered.\nShould we include API endpoints and database schema here?\nThis depends on the problem. For large design problems like “Design Google search engine”, this is a bit of too low level. For a problem like designing the backend for a multi-player poker game, this is a fair game. Communicate with your interviewer.\nStep 3 Design Deep Dive At this step, you and your interviewer should have already achieved the following objectives:\nAgreed on the overall goals and feature scope Sketched out a high-level blueprint for the overall design Obtained feedback from your interviewer on the high-level design Had some initial ideas about areas to focus on in deep dive based on her feedback You shall work with the interviewer to identify and prioritize components in the architecture. It is worth stressing that every interview is different.\nSometimes, the interviewer may give off hints that she likes focusing on high-level design. Sometimes, for a senior candidate interview, the discussion could be on the system performance characteristics, likely focusing on the bottlenecks and resource estimations. In most cases, the interviewer may want you to dig into details of some system components. For URL shortener, it is interesting to dive into the hash function design that converts a long URL to a short one. For a chat system, how to reduce latency and how to support online/offline status are two interesting topics. Time management is essential as it is easy to get carried away with minute details that do not demonstrate your abilities. You must be armed with signals to show your interviewer. Try not to get into unnecessary details. For example, talking about the EdgeRank algorithm of Facebook feed ranking in detail is not ideal during a system design interview as this takes much precious time and does not prove your ability in designing a scalable system.\nStep 4 Wrap up In this final step, the interviewer might ask you a few follow-up questions or give you the freedom to discuss other additional points. Here are a few directions to follow:\nThe interviewer might want you to identify the system bottlenecks and discuss potential improvements. Never say your design is perfect and nothing can be improved. There is always something to improve upon. This is a great opportunity to show your critical thinking and leave a good final impression. It could be useful to give the interviewer a recap of your design. This is particularly important if you suggested a few solutions. Refreshing your interviewer’s memory can be helpful after a long session. Error cases (server failure, network loss, etc.) are interesting to talk about. Operation issues are worth mentioning. How do you monitor metrics and error logs? How to roll out the system? How to handle the next scale curve is also an interesting topic. For example, if your current design supports 1 million users, what changes do you need to make to support 10 million users? Propose other refinements you need if you had more time. To wrap up, we summarize a list of the Dos and Don’ts.\nDos\nAlways ask for clarification. Do not assume your assumption is correct. Understand the requirements of the problem. There is neither the right answer nor the best answer. A solution designed to solve the problems of a young startup is different from that of an established company with millions of users. Make sure you understand the requirements. Let the interviewer know what you are thinking. Communicate with your interview. Suggest multiple approaches if possible. Once you agree with your interviewer on the blueprint, go into details on each component. Design the most critical components first. Bounce ideas off the interviewer. A good interviewer works with you as a teammate. Never give up. Don’ts\nDon\u0026rsquo;t be unprepared for typical interview questions. Don’t jump into a solution without clarifying the requirements and assumptions. Don’t go into too much detail on a single component in the beginning. Give the high- level design first then drills down. If you get stuck, don\u0026rsquo;t hesitate to ask for hints. Again, communicate. Don\u0026rsquo;t think in silence. Don’t think your interview is done once you give the design. You are not done until your interviewer says you are done. Ask for feedback early and often. Time allocation on each step System design interview questions are usually very broad, and 45 minutes or an hour is not enough to cover the entire design. Time management is essential. How much time should you spend on each step? The following is a very rough guide on distributing your time in a 45- minute interview session. Please remember this is a rough estimate, and the actual time distribution depends on the scope of the problem and the requirements from the interviewer.\nStep 1 Understand the problem and establish design scope: 3 - 10 minutes Step 2 Propose high-level design and get buy-in: 10 - 15 minutes Step 3 Design deep dive: 10 - 25 minutes Step 4 Wrap: 3 - 5 minutes\n","date":"2024-03-23T11:50:35Z","image":"https://blog.xiax.xyz/p/system-design-interview-framework/banner_hu6bdf848fc2e03cb76a7cc594088d1a18_89181_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.xiax.xyz/p/system-design-interview-framework/","title":"System Design Interview Framework"}]